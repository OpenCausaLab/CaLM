target_overview_dict = [
{'Index': "Basic Prompt", 'Level': 'None', 'Average': 29.4, 'description': "The primary purpose of the basic prompt is to offer foundational and unambiguous test sets to LLMs that are applicable across different scenarios. This prompt will not provide any example or instruction, it only gives the question as the input string directly. Not only does this establish a clear starting point, but it also paves the way for subsequent modifications and iterations of the prompt to cater to specific needs. This not only establishes a clear starting point, but also lays the groundwork for making subsequent modifications and iterations to tailor the prompt to specific needs.<br>We implement a uniform interactive method across all scenarios. In binary classification (e.g. yes or no questions), the model is prompted to provide a direct classification response. For multiple-choice questions, all options are presented (similar to standard exam formats), requiring the model to make a selection. In math computations, the model is tasked with directly calculating the probability outcome. In generation tasks, the model produces the answer straightforwardly. These formats demonstrate versatility and strong generalization, consistent with prior research. And the above strategies are the same across all prompts, Figure x gives a detailed illustration. ", 'imageUrl': ""},
{'Index': "Adversarial Prompt", 'Level': 'None', 'Average': 28.0, 'description': "Adversarial prompt is pivotal in comprehending the inherent risks associated with LLMs. Our intent is not to endorse malicious activities directed towards LLMs. Instead, our aim is to delve deeper into their potential shortcomings, thereby facilitating the development of more robust and secure LLMs in the future.<br>We employ two distinct forms of adversarial prompts. (1) The first is a subtler approach, which compels LLMs to ignore the answers they previously provided. (2) The second is a more assertive form, where the LLMs are explicitly informed that their initial responses were wrong. Due to the fact that some models do not offer interfaces for multi-turn dialogue, for the sake of evaluation consistency, we adopted the following approach. We first pose a question to the model and record model output 1. For the second inquiry, we inform the model of output 1 (i.e. the one just recorded) and utilize the adversarial prompts for interference. Subsequently, we present the same question again, obtaining the model output 2. The responses from the two instances represent the pre- and post-adversarial conditions. By comparing these responses, we can gain a deeper understanding of the model's robustness and accuracy. The underlying consequence of both prompts is that they can instill doubt in LLMs about their original responses. This, in turn, may lead them to produce an inaccurate answer. An interesting observation from this process is the insight it offers into the confidence level of LLMs regarding their responses. Essentially, if the model's answer varies significantly post-adversarial interference, it implies a lower level of assurance in its original answer. Conversely, minimal changes suggest higher confidence in its initial response.", 'imageUrl': "../figures/adversarial.png"},
{'Index': "Chain-of-Thought", 'Level': 'None', 'Average': 32.2, 'description': "Chain-of-Thought (CoT) prompting enables large language models to decompose complex problems and perform intermediate reasoning steps to enhance their performance. Previous studies have demonstrated that CoT prompts outperform basic prompts on sufficiently large models, particularly on complex arithmetic, commonsense, and symbolic reasoning tasks. ", 'imageUrl': "../figures/cot.png"},
{'Index': "In-Context Learning", 'Level': 'None', 'Average': 35.3, 'description': "In-Context Learning (IcL) represents a technique whereby a model learns new tasks through a set of examples within the context of the prompt provided at the inference phase. The fundamental concept of IcL is learning from analogy, allowing the model to generalize from a limited set of input-output examples. Such learning ability is also recognized as an emerging ability that particularly appears in large language models.", 'imageUrl': "../figures/icl.png"},
{'Index': "Explicit Function", 'Level': 'None', 'Average': 33.2, 'description': "Recent studies have elucidated that LLMs may have emotional awareness analogous to humans. Derived from this understanding, several related works have explored the utilization of encouraging and positive language within prompts (e.g., statements that build confidence or emphasize the goal) to elicit enhanced performance from LLMs. In our work, we formulate an explicit function prompt for each task. Specifically, we incorporate a sentence containing an explicit function description into the basic prompt to motivate LLMs in task resolution.", 'imageUrl': "../figures/ef.png"},
]