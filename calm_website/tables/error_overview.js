error_overview_dict = [
{'Index': 1, "Name":"Same Response to All Questions", "Class": "Quantitative", 'Level': 'None', 'description': "This type of error refers to cases where a model consistently produces the same answer regardless of the specific question posed. Such errors can be differentiated based on the question type, consisting of three categories and five situations: (1) In binary classification, responses can be ''all yes'', ''all no'', or a mix of both across different prompts; (2) In choice selection, the same choice might be selected in all queries; (3) In probability calculation, a specific value might be consistently generated in every case. It is worth noting that some models persist in giving the same answer even when presented with adversarial prompts. Cataloging this error is important because it indicates a lack of adaptability and contextual understanding in the model's responses, and this kind of error may result in a specious high accuracy value in some causal scenarios. By consistently providing the same answer regardless of the input, the model fails to demonstrate versatility in its decision-making process. This error hampers the model's ability to effectively handle diverse causal scenarios and causal tasks, undermining its overall performance and reliability.", 'imageUrl': "", "reference":""},
{'Index': 1, "Name":"Empty Response", "Class": "Quantitative", 'Level': 'None', 'description': "This type of error refers to instances where a model generates a blank response. Identifying this error is beneficial for several reasons. First, it helps us identify the boundaries of the model's capabilities. When the model fails to produce any output, it indicates limitations in its understanding or processing of the input data. Second, it highlights potential areas for improvement in the model's architecture or training process. Additionally, documenting instances of blank responses allows us to assess the overall reliability and robustness of the model. By identifying and addressing the root causes of this error, we can work towards enhancing the model's performance and ensuring its effectiveness in practical applications.", 'imageUrl': "../figures/error/quanti_empty.png", "reference":""},
{'Index': 1, "Name":"Limitation of Instruction-Following", "Class": "Quantitative", 'Level': 'None', 'description': "This type of error occurs when a model fails to provide a standard response according to the instructions given in the question. For example, when asked to directly respond with ''Yes'' or ''No'', some models may provide explanations instead, with the answer embedded within the explanation. This type of response introduces additional inconvenience for our metric calculations. Alternatively, some models may choose to reply with ''true'' or ''false''. In probability calculation problems, we instruct the model to return the answer in a specific JSON format (e.g., \{''PROB'': ''0.1234''\}). However, some models may provide the probability directly without adhering to the required format specified in the question. The significance of cataloging this type of error lies in several aspects. Firstly, it helps us evaluate the model's adherence to instructions and its ability to respond in the desired format. Moreover, it will greatly facilitate large-scale evaluations and bring considerable convenience to metric calculations. By minimizing these errors, we can streamline the evaluation process and ensure the reliability and efficiency of our assessment metrics, ultimately enhancing the robustness and effectiveness of model evaluations on a broader scale.", 'imageUrl': "../figures/error/quanti_limitation.png", "reference":""},
{'Index': 1, "Name":"Repetition", "Class": "Quantitative", 'Level': 'None', 'description': "This type of error, where a model fabricates responses or repeats questions within its replies, presents significant challenges to our evaluation process. For example, there are cases where the model starts repeating fabricated questions, following with a ''Yes'' or ''No'' answer as mentioned in the instructions. This requires special handling during keyword matching to accurately discover the answer section. Tracking and quantifying these errors are crucial as they offer insights into the model's behavior and performance, thus enabling the implementation of necessary improvements. Moreover, understanding the prevalence of these errors aids in refining the evaluation process, and ensuring accurate assessment of the model's capabilities.", 'imageUrl': "../figures/error/quanti_repetition.png", "reference":""},
{'Index': 1, "Name":"Language Inconsistency", "Class": "Quantitative", 'Level': 'None', 'description': "Our evaluation process provides questions in both Chinese and English versions, but never simultaneously in two languages. Under this setup, we have identified instances of language inconsistency in certain models. This issue arises when a question posed in one language elicits a response that includes text in the other language. It is more common to find English text within Chinese responses. This error not only reflects an imbalance in the model's training data across different languages, but also signifies a deficiency in the model's ability to recognize and integrate the appropriate language context. Furthermore, it underscores the importance of multilingual training data and fine-tuning methodologies to ensure coherent and linguistically appropriate responses across languages. Addressing this language inconsistency is crucial for enhancing the model's cross-lingual capabilities and overall performance in diverse linguistic environments.", 'imageUrl': "../figures/error/quanti_language.png", "reference":""},
{'Index': 2, "Name":"Causal Hallucination", "Class": "Qualitative", 'Level': 'None', 'description': "As defined by [1], causal hallucination refers to a model's inability to correctly distinguish between two fundamental concepts: correlation and causation. The model may mistakenly interpret correlation as causation, leading to erroneous reasoning and conclusions. Causal hallucination may arise due to various factors, including limited data availability, complexity in relationships between variables, the presence of confounding variables, biases within the data, and insufficient domain knowledge. Overcoming causal hallucination requires comprehensive strategies such as accounting for confounding variables, validating assumptions, and leveraging domain expertise to ensure the model accurately captures causal relationships. ", 'imageUrl': "../figures/error/quali_hallucination.png","reference":"<h2>References</h2>[1] Lu, C., Qian, C., Zheng, G., Fan, H., Gao, H., Zhang, J., Shao, J., Deng, J., Fu, J., Huang, K., et al. From gpt-4 to gemini and beyond: Assessing the landscape of mllms on generalizability, trustworthiness and causality through four modalities. arXiv preprint arXiv:2401.15071, 2024."},
{'Index': 2, "Name":"Inferential Ambiguity", "Class": "Qualitative", 'Level': 'None', 'description': "This type of error occurs when a model, despite being presented with a solvable problem, produces an overly broad or vague answer, making it difficult to determine its intent. Such errors typically indicate deficiencies in the model's data processing abilities or semantic understanding, suggesting a need for improvement in its reasoning or comprehension capabilities. Addressing this issue is vital for improving the model's accuracy and reliability, ensuring that its responses are more precise and contextually relevant. ", 'imageUrl': "../figures/error/quali_ambiguity.png","reference":""},
{'Index': 2, "Name":"Calculation Error", "Class": "Qualitative", 'Level': 'None', 'description': "We categorize this type of error as occurring when a model understands the question semantically and engages in basic reasoning but errs during the calculation phase. This error also occurs in [1]. Numerous studies have highlighted that computation is extremely challenging to models<sup>[2,3,4]</sup>. In CaLM, the inclusion of a causal background introduces additional complexity to them. It is of vital importance for a model to compute accurately, because it impacts the reliability and usefulness of its outputs. In domains where causality plays a significant role, such as healthcare decision-making<sup>[5]</sup>, economics<sup>[6]</sup>, or policy development<sup>[7]</sup>, precise computation is paramount. Errors in computation could lead to incorrect conclusions, flawed recommendations, or even harmful actions. ", 'imageUrl': "../figures/error/quali_calculation.png", "reference":"<h2>References</h2>[1] Sawada, T., Paleka, D., Havrilla, A., Tadepalli, P., Vidas, P., Kranias, A., Nay, J., Gupta, K., and Komatsuzaki, A. Arb: Advanced reasoning benchmark for large language models. In The 3rd Workshop on Mathematical Reasoning and AI at NeurIPS’23, 2023.<br>[2] He-Yueya, J., Poesia, G., Wang, R., and Goodman, N. Solving math word problems by combining language models with symbolic solvers. In The 3rd Workshop on Mathematical Reasoning and AI at NeurIPS’23, 2023.<br>[3] Zhang, B., Zhou, K., Wei, X., Zhao, X., Sha, J., Wang, S., and Wen, J.-R. Evaluating and improving tool-augmented computation-intensive math reasoning. Advances in Neural Information Processing Systems, 36, 2024.<br>[4] Zhou, Z., Wang, Q., Jin, M., Yao, J., Ye, J., Liu, W., Wang, W., Huang, X., and Huang, K. Mathattack: Attacking large language models towards math solving ability. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 19750–19758, 2024.<br>[5] Richens, J. G., Lee, C. M., and Johri, S. Improving the accuracy of medical diagnosis with causal machine learning. Nature communications, 11(1):3923, 2020.<br>[6] Uysal, S. D. Doubly robust estimation of causal effects with multivalued treatments: an application to the returns to schooling. Journal of Applied Econometrics, 30(5):763–786, 2015.<br>[7] Capano, G. and Howlett, M. Causal logics and mechanisms in policy design: How and why adopting a mechanistic perspective can improve policy design. Public policy and administration, 36(2):141–162, 2021."},
{'Index': 2, "Name":"Incorrect Reasoning", "Class": "Qualitative", 'Level': 'None', 'description': "This type of error occurs when a model makes a mistake during the reasoning process, specifically using the Chain-of-Thought (CoT), and fails to arrive at the correct conclusion. Usually, not every step in the model-generated CoT is necessary for answering a question, and even some incorrect steps may not impact the final outcome. However, errors in critical reasoning steps will invariably lead to incorrect conclusions. Given our goal for the model to exhibit robust causal reasoning capabilities, it is crucial to ensure the accuracy of each step in the model's reasoning process. Identifying strategies to enhance the model's deductive accuracy remains a significant challenge.", 'imageUrl': "../figures/error/quali_indirect.png", "reference":""},
{'Index': 2, "Name":"Misunderstanding", "Class": "Qualitative", 'Level': 'None', 'description': "This type of error occurs when a model misunderstands the input, producing content that, while related to the input, is irrelevant to the ground truth answer. Such errors can be particularly severe, especially in real-world causal scenarios. For instance, if the model inappropriately responds to a query, it will inevitably impair the user experience. Encountering responses that are tangentially related but ultimately irrelevant not only erodes trust in the technology but also obstructs the adoption and integration of language models into routine causal tasks and decision-making processes. This highlights the critical need for continuous improvement in model accuracy and understanding.", 'imageUrl': "../figures/error/quali_misunderstanding.png", "reference":""},
{'Index': 2, "Name":"Contradiction", "Class": "Qualitative", 'Level': 'None', 'description': "This type of error arises from contradictions within a model's responses. Specifically, when faced with a ''Yes'' or ''No'' query, the model produces both ''Yes'' and ''No'' simultaneously. Similarly, for multiple-choice questions with only one correct option, the model may suggest several choices concurrently. These issues reveal a fundamental flaw: the model's inability to maintain coherence and rigor in decision-making. Such contradictions not only confuse users but also undermine the model's reliability and its applicability in critical decision-making causal scenarios. The root cause of this issue can be traced back to the model's processing and evaluation mechanism, which, in attempting to cover a broad spectrum of possibilities, fails to adequately weigh the context and nuances of the query. Consequently, the model defaults to presenting multiple outcomes without a clear rationale for prioritizing one over the others. This behavior suggests a need for improvement in the model's understanding of the query's context and its decision-making algorithms, to enhance its ability to provide precise and unambiguous answers.", 'imageUrl': "../figures/error/quali_contradiction.png", "reference":""},
{'Index': 2, "Name":"Outlier", "Class": "Qualitative", 'Level': 'None', 'description': "This type of error indicates a complete failure by a model to understand the intended request of the input, leading to generated content that bears no relation to the input provided. It is important to differentiate this error from <b>Misunderstanding</b>, as they can be easily confused. <b>Misunderstanding</b> refers to an erroneous interpretation of the input, where the response retains a degree of association with the input. In contrast, <b>Outlier</b> represent such a significant deviation that the response is completely disconnected from the input. This issue highlights a fundamental limitation within the model: its inability to understand and process context. These limitations are inherently tied to the model's training data and the algorithm's capacity to interpret context and meaning from that data. Errors of this nature may stem from the model's inability to accurately map complex inputs to its learned representations, resulting in outputs that are not only incorrect but completely irrelevant. This limitation underscores the challenges in embedding human-like understanding and interpretive flexibility into a system primarily based on pattern recognition and causal reasoning. It reveals the gap between algorithmic processing and human cognition, particularly in dealing with ambiguous, nuanced, or highly contextualized information.", 'imageUrl': "../figures/error/quali_outlier.png", "reference":""},
]